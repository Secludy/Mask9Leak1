{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import random\n",
    "import re\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize vLLM model\n",
    "llm = LLM(\n",
    "    model=\"/path_to_your_model/Qwen/Qwen2.5-7B-Instruct\",\n",
    "    tensor_parallel_size=1,  # Adjust based on your GPU setup\n",
    "    trust_remote_code=True,\n",
    "    max_model_len=2048\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=2048,\n",
    "    repetition_penalty=1.2  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import random\n",
    "import re\n",
    "from vllm import LLM, SamplingParams\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import math\n",
    "import warnings\n",
    "import logging\n",
    "from transformers import logging as transformers_logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Silence the warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "transformers_logging.set_verbosity_error()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize BERT pipeline once\n",
    "ner_model = pipeline(\"ner\", \n",
    "                    model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "                    aggregation_strategy=\"simple\",\n",
    "                    device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def step1_bert_ner(text):\n",
    "    \"\"\"\n",
    "    Step 1: Use BERT to identify and mask named entities, plus regex for additional PII\n",
    "    \"\"\"\n",
    "    # Update regex patterns to catch numbers without requiring prefixes\n",
    "    pii_patterns = {\n",
    "        'SSN': r'\\b\\d{3}[-\\s]?\\d{2}[-\\s]?\\d{4}\\b',  # Just the SSN format\n",
    "        'VIN': r'\\b[A-HJ-NPR-Z0-9]{17}\\b',  # 17 alphanumeric chars\n",
    "        'BTC': r'\\b[A-Za-z0-9]{20,}(?=[^A-Za-z0-9]|$)',\n",
    "        'DL': r'\\b[A-Z]\\d{7}\\b',  # Letter followed by 7 digits\n",
    "        'EMAIL': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    }\n",
    "    \n",
    "    # Process regex patterns first\n",
    "    masked_text = text\n",
    "    mapping = {}\n",
    "    entity_counts = {}\n",
    "    \n",
    "    # Handle regex patterns first\n",
    "    for pii_type, pattern in pii_patterns.items():\n",
    "        matches = list(re.finditer(pattern, masked_text))\n",
    "        matches.reverse()  # Process from end to start\n",
    "        \n",
    "        for match in matches:\n",
    "            entity_counts[pii_type] = entity_counts.get(pii_type, 0) + 1\n",
    "            mask = f\"[{pii_type}{entity_counts[pii_type]}]\"\n",
    "            \n",
    "            masked_text = (\n",
    "                masked_text[:match.start()] + \n",
    "                mask + \n",
    "                masked_text[match.end():]\n",
    "            )\n",
    "            \n",
    "            mapping[mask] = {\n",
    "                'original': match.group(),\n",
    "                'type': pii_type\n",
    "            }\n",
    "    \n",
    "    # Then process BERT entities\n",
    "    entities = ner_model(masked_text)\n",
    "    \n",
    "    # Sort entities by start position in reverse order\n",
    "    entities.sort(key=lambda x: x['start'], reverse=True)\n",
    "    \n",
    "    # Process BERT entities\n",
    "    for entity in entities:\n",
    "        entity_type = entity['entity_group']\n",
    "        if entity_type not in ['PER', 'ORG', 'LOC']:\n",
    "            continue\n",
    "            \n",
    "        entity_counts[entity_type] = entity_counts.get(entity_type, 0) + 1\n",
    "        mask = f\"[{entity_type}{entity_counts[entity_type]}]\"\n",
    "        \n",
    "        masked_text = (\n",
    "            masked_text[:entity['start']] + \n",
    "            mask + \n",
    "            masked_text[entity['end']:]\n",
    "        )\n",
    "        \n",
    "        mapping[mask] = {\n",
    "            'original': entity['word'],\n",
    "            'type': entity_type\n",
    "        }\n",
    "    \n",
    "    return masked_text, mapping\n",
    "\n",
    "def get_random_pii(entity_type, *args):\n",
    "    \"\"\"\n",
    "    Fallback function for random PII generation with more options\n",
    "    \"\"\"\n",
    "    random_values = {\n",
    "        'PER': [\n",
    "            'James Wilson', 'Mary Johnson', 'Robert Brown', 'Sarah Davis',\n",
    "            'Michael Chen', 'Emily Taylor', 'David Miller', 'Lisa Anderson',\n",
    "            'Thomas Wright', 'Jennifer Lee'\n",
    "        ],\n",
    "        'ORG': [\n",
    "            'Acme Corp', 'Global Tech', 'Summit Industries', 'Pioneer Systems',\n",
    "            'Blue Ridge Solutions', 'Nexus Innovations', 'Quantum Dynamics',\n",
    "            'Atlas Technologies', 'Stellar Enterprises', 'Horizon Group'\n",
    "        ],\n",
    "        'LOC': [\n",
    "            'Chicago', 'Los Angeles', 'Boston', 'Seattle',\n",
    "            'Austin', 'Denver', 'Portland', 'Miami',\n",
    "            'Atlanta', 'San Francisco', 'Dallas', 'Phoenix'\n",
    "        ],\n",
    "        'SSN': [\n",
    "            '123-45-6789', '987-65-4321', '456-78-9012', '789-01-2345'\n",
    "        ],\n",
    "        'DL': [\n",
    "            'A1234567', 'B7654321', 'C9876543', 'D2468101'\n",
    "        ],\n",
    "        'VIN': [\n",
    "            '1HGCM82633A123456', '2FMDK3JC8BB234567', '3VWFE21C04M345678'\n",
    "        ],\n",
    "        'BTC': [\n",
    "            '5JWwqjxTLBcJig6SgfiksxY6C1XEwE9ZJGEeyRb8K8N1qP3Xm5n',\n",
    "            '5KQNQKj2FEsNvnXXBF73J7kMJqg3hdY6yQwkRQ9hNPPuPKZNiBk'\n",
    "        ],\n",
    "        'EMAIL': [\n",
    "            'user1@example.com',\n",
    "            'contact@company.net',\n",
    "            'info@business.org',\n",
    "            'support@service.com'\n",
    "        ]\n",
    "    }\n",
    "    return random.choice(random_values[entity_type])\n",
    "\n",
    "def clean_mistral_output(text, entity_type):\n",
    "    \"\"\"\n",
    "    Clean up Mistral output and validate based on entity type\n",
    "    \"\"\"\n",
    "    # Remove instruction tokens and anything after them\n",
    "    if '[/INST]' in text:\n",
    "        text = text.split('[/INST]')[0]\n",
    "    \n",
    "    # Remove any special tokens or prefixes\n",
    "    text = text.replace('[INST]', '').strip()\n",
    "    text = re.sub(r'^(Location:|Locality:|Name:|Organization:|Company:|You are)\\s*', '', text)\n",
    "    \n",
    "    # Remove any text after common separators\n",
    "    separators = ['/STATE', '/', ',', ' - ', ' You ']\n",
    "    for sep in separators:\n",
    "        if sep in text:\n",
    "            text = text.split(sep)[0]\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    # Validate based on entity type\n",
    "    if entity_type == 'PER':\n",
    "        # Take only first and last name if more than two words\n",
    "        words = text.split()\n",
    "        if len(words) > 2:\n",
    "            text = f\"{words[0]} {words[-1]}\"\n",
    "        # Ensure it looks like a person name (2 words, no numbers)\n",
    "        if not re.match(r'^[A-Za-z]+\\s+[A-Za-z]+$', text):\n",
    "            return ''\n",
    "            \n",
    "    elif entity_type == 'ORG':\n",
    "        # Ensure it's not too long and doesn't contain special characters\n",
    "        if len(text) > 30 or re.search(r'[^\\w\\s&\\'-]', text):\n",
    "            return ''\n",
    "            \n",
    "    elif entity_type == 'LOC':\n",
    "        # Ensure it's a simple location name\n",
    "        if len(text) > 20 or re.search(r'[^\\w\\s\\'-]', text):\n",
    "            return ''\n",
    "        # Ensure it's not just \"New\" or generic terms\n",
    "        if text.lower() in ['new', 'location', 'city', 'town']:\n",
    "            return ''\n",
    "    \n",
    "    # Add validation for new PII types\n",
    "    elif entity_type == 'SSN':\n",
    "        # Ensure it matches SSN format\n",
    "        if not re.match(r'^\\d{3}-?\\d{2}-?\\d{4}$', text):\n",
    "            return ''\n",
    "    elif entity_type == 'DL':\n",
    "        # Ensure it matches driver's license format\n",
    "        if not re.match(r'^[A-Z]\\d{7}$', text):\n",
    "            return ''\n",
    "    elif entity_type == 'VIN':\n",
    "        # Ensure it matches VIN format\n",
    "        if not re.match(r'^[A-HJ-NPR-Z0-9]{17}$', text):\n",
    "            return ''\n",
    "    elif entity_type == 'BTC':\n",
    "        # Ensure it matches Bitcoin address format\n",
    "        if not re.match(r'^[1-9A-HJ-NP-Za-km-z]{51,52}$', text):\n",
    "            return ''\n",
    "    elif entity_type == 'EMAIL':\n",
    "        if not re.match(r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}$', text):\n",
    "            return ''\n",
    "    \n",
    "    return text\n",
    "\n",
    "def step2_mistral_replace(masked_text, mapping):\n",
    "    \"\"\"\n",
    "    Step 2: Use Mistral to generate replacements\n",
    "    \"\"\"\n",
    "    for mask, info in mapping.items():\n",
    "        try:\n",
    "            replacement = get_context_aware_replacement(\n",
    "                info['type'], \n",
    "                mask, \n",
    "                masked_text\n",
    "            )\n",
    "            \n",
    "            # Clean up the replacement text\n",
    "            replacement = clean_mistral_output(replacement, info['type'])\n",
    "            \n",
    "            if replacement and len(replacement.strip()) > 0:\n",
    "                mapping[mask]['replacement'] = replacement\n",
    "                masked_text = masked_text.replace(mask, replacement)\n",
    "            else:\n",
    "                # Fallback to random PII if Mistral output is invalid\n",
    "                fallback = get_random_pii(info['type'])\n",
    "                mapping[mask]['replacement'] = fallback\n",
    "                masked_text = masked_text.replace(mask, fallback)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error replacing {mask}, using fallback: {str(e)}\")\n",
    "            fallback = get_random_pii(info['type'])\n",
    "            mapping[mask]['replacement'] = fallback\n",
    "            masked_text = masked_text.replace(mask, fallback)\n",
    "    \n",
    "    return masked_text, mapping\n",
    "\n",
    "def step3_save_mapping(mapping, filename=\"entity_mapping.json\"):\n",
    "    \"\"\"\n",
    "    Step 3: Save the mapping to a file\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(mapping, f, indent=2)\n",
    "    return filename\n",
    "\n",
    "def step4_restore_text(text, mapping):\n",
    "    \"\"\"\n",
    "    Step 4: Restore original text using mapping\n",
    "    \"\"\"\n",
    "    restored_text = text\n",
    "    for mask, info in mapping.items():\n",
    "        restored_text = restored_text.replace(info['replacement'], info['original'])\n",
    "    return restored_text\n",
    "\n",
    "\n",
    "def get_random_different_pii(entity_type, original_value):\n",
    "    \"\"\"\n",
    "    Get a random PII value that's different from the original\n",
    "    \"\"\"\n",
    "    random_values = {\n",
    "        'PER': [\n",
    "            'James Wilson', 'Mary Johnson', 'Robert Brown', 'Sarah Davis',\n",
    "            'Michael Chen', 'Emily Taylor', 'David Miller', 'Lisa Anderson'\n",
    "        ],\n",
    "        'ORG': [\n",
    "            'Acme Corp', 'Global Tech', 'Summit Industries', 'Pioneer Systems',\n",
    "            'Blue Ridge Solutions', 'Nexus Innovations', 'Quantum Dynamics'\n",
    "        ],\n",
    "        'LOC': [\n",
    "            'Chicago', 'Los Angeles', 'Boston', 'Seattle',\n",
    "            'Austin', 'Denver', 'Portland', 'Miami'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    options = [v for v in random_values[entity_type] if v.lower() != original_value.lower()]\n",
    "    return random.choice(options)\n",
    "\n",
    "class PIITracker:\n",
    "    def __init__(self):\n",
    "        self.pii_sets = {\n",
    "            'PER': {},  # {original_text: set_id}\n",
    "            'ORG': {},\n",
    "            'LOC': {}\n",
    "        }\n",
    "        self.current_sets = {\n",
    "            'PER': 0,\n",
    "            'ORG': 0,\n",
    "            'LOC': 0\n",
    "        }\n",
    "        self.replacements = {}  # {set_id: replacement_value}\n",
    "    \n",
    "    def identify_pii(self, original_text, entity_type):\n",
    "        \"\"\"\n",
    "        Identify if PII belongs to existing set or create new set\n",
    "        \"\"\"\n",
    "        pii_dict = self.pii_sets[entity_type]\n",
    "        \n",
    "        # Check if this exact text was seen before\n",
    "        if original_text in pii_dict:\n",
    "            return pii_dict[original_text]\n",
    "        \n",
    "        # Check if this text might be a variant of existing PII\n",
    "        for known_text, set_id in pii_dict.items():\n",
    "            if self._are_similar(original_text, known_text):\n",
    "                pii_dict[original_text] = set_id\n",
    "                return set_id\n",
    "        \n",
    "        # Create new set if not found\n",
    "        self.current_sets[entity_type] += 1\n",
    "        set_id = f\"{entity_type}_{self.current_sets[entity_type]}\"\n",
    "        pii_dict[original_text] = set_id\n",
    "        return set_id\n",
    "    \n",
    "    def _are_similar(self, text1, text2):\n",
    "        \"\"\"\n",
    "        Check if two pieces of text might be variants of the same PII\n",
    "        \"\"\"\n",
    "        # Convert to lowercase and remove common prefixes/suffixes\n",
    "        t1 = text1.lower().strip()\n",
    "        t2 = text2.lower().strip()\n",
    "        \n",
    "        # Direct match\n",
    "        if t1 == t2:\n",
    "            return True\n",
    "        \n",
    "        # One is contained in the other\n",
    "        if t1 in t2 or t2 in t1:\n",
    "            return True\n",
    "        \n",
    "        # TODO: Could add more sophisticated matching here\n",
    "        # (e.g., fuzzy matching, nickname matching for persons)\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def add_replacement(self, set_id, replacement):\n",
    "        \"\"\"\n",
    "        Add or get consistent replacement for a PII set\n",
    "        \"\"\"\n",
    "        if set_id not in self.replacements:\n",
    "            self.replacements[set_id] = replacement\n",
    "        return self.replacements[set_id]\n",
    "    \n",
    "    def get_pii_report(self):\n",
    "        \"\"\"\n",
    "        Generate report of PII sets and their replacements\n",
    "        \"\"\"\n",
    "        report = []\n",
    "        for entity_type in self.pii_sets:\n",
    "            for original, set_id in self.pii_sets[entity_type].items():\n",
    "                replacement = self.replacements.get(set_id, \"NOT_REPLACED\")\n",
    "                report.append({\n",
    "                    'set_id': set_id,\n",
    "                    'type': entity_type,\n",
    "                    'original': original,\n",
    "                    'replacement': replacement\n",
    "                })\n",
    "        return report\n",
    "\n",
    "def group_entities(entities):\n",
    "    \"\"\"\n",
    "    Group entities based on their position in text.\n",
    "    \"\"\"\n",
    "    grouped = []\n",
    "    current_group = None\n",
    "    last_end = -1\n",
    "    \n",
    "    for entity in entities:\n",
    "        if not entity:  # Skip if entity is None\n",
    "            continue\n",
    "            \n",
    "        entity_type = entity['entity'].replace('I-', '').replace('B-', '')  # Remove I- and B- prefixes\n",
    "        \n",
    "        # Start new group if not adjacent to previous entity or different type\n",
    "        if (current_group is None or \n",
    "            entity['start'] > last_end + 1 or \n",
    "            entity_type != current_group['type']):\n",
    "            \n",
    "            if current_group:\n",
    "                grouped.append(current_group)\n",
    "            current_group = {\n",
    "                'word': entity['word'],\n",
    "                'type': entity_type,\n",
    "                'start': entity['start'],\n",
    "                'end': entity['end']\n",
    "            }\n",
    "        else:\n",
    "            # Extend current group\n",
    "            current_group['word'] += ' ' + entity['word']\n",
    "            current_group['end'] = entity['end']\n",
    "        \n",
    "        last_end = entity['end']\n",
    "    \n",
    "    # Add last group\n",
    "    if current_group:\n",
    "        grouped.append(current_group)\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "def apply_fallback_replacements(text, pii_tracker):\n",
    "    \"\"\"\n",
    "    Apply fallback replacements using BERT NER with grouped entities\n",
    "    \"\"\"\n",
    "    fallback_values = {\n",
    "        'PER': ['Emily Thompson', 'Michael Chen', 'Sarah Davis', 'Robert Wilson'],\n",
    "        'ORG': ['TechCorp', 'Quantum Systems', 'Pioneer Solutions', 'Global Dynamics'],\n",
    "        'LOC': ['Boston', 'Seattle', 'Chicago', 'Austin']\n",
    "    }\n",
    "    \n",
    "    # Use BERT to identify entities\n",
    "    ner_model = pipeline(\"ner\", \n",
    "                        model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "                        aggregation_strategy=\"simple\")  # Use simple aggregation\n",
    "    \n",
    "    raw_entities = ner_model(text)\n",
    "    \n",
    "    # Sort entities by start position in reverse order\n",
    "    raw_entities.sort(key=lambda x: x['start'], reverse=True)\n",
    "    \n",
    "    result = text\n",
    "    for entity in raw_entities:\n",
    "        entity_type = entity['entity_group']  # Using entity_group from simple aggregation\n",
    "        if entity_type in ['PER', 'ORG', 'LOC']:\n",
    "            original = entity['word']\n",
    "            set_id = pii_tracker.identify_pii(original, entity_type)\n",
    "            replacement = pii_tracker.replacements.get(set_id)\n",
    "            \n",
    "            if not replacement:\n",
    "                replacement = random.choice(fallback_values[entity_type])\n",
    "                pii_tracker.add_replacement(set_id, replacement)\n",
    "            \n",
    "            # Replace in text\n",
    "            result = (\n",
    "                result[:entity['start']] + \n",
    "                replacement + \n",
    "                result[entity['end']:]\n",
    "            )\n",
    "    \n",
    "    return result\n",
    "\n",
    "def clean_output(text):\n",
    "    \"\"\"Clean the model output\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # Remove any lines starting with ###\n",
    "    text = re.sub(r'\\n###.*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove any \"Replacements:\" section\n",
    "    text = re.sub(r'\\n?Replacements:.*$', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove any \"Note how\" explanatory text\n",
    "    text = re.sub(r'\\n?Note how.*$', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove any trailing response markers\n",
    "    text = re.sub(r'\\n?### Response:.*$', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Clean up any extra spaces in email addresses\n",
    "    text = re.sub(r'(\\[[\\w.]+)\\s*\\.\\s*([\\w]+@[\\w.]+\\])', r'\\1.\\2', text)\n",
    "    \n",
    "    # Remove any explanatory notes at the end\n",
    "    text = re.sub(r'\\n?---.*$', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    return text.strip()\n",
    "def replace_entities_with_mistral(text, bracketed_text, pii_tracker=None):\n",
    "    \"\"\"\n",
    "    Direct entity replacement using vLLM with Alpaca-style prompt\n",
    "    \"\"\"\n",
    "    if pii_tracker is None:\n",
    "        pii_tracker = PIITracker()\n",
    "        \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a specialized entity replacement system. Replace ALL marked entities in [brackets] with new consistent values.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Replace ALL entities marked with [brackets] in this text. Each entity MUST be replaced.\n",
    "\n",
    "### Input:\n",
    "{bracketed_text}\n",
    "### Examples:\n",
    "Input: [John Smith] works at [Microsoft]. [John] enjoys [Seattle]. At [Microsoft], [Smith] is happy.\n",
    "Output: [Michael Brown] works at [TechCorp]. [Michael] enjoys [Boston]. At [TechCorp], [Brown] is happy.\n",
    "\n",
    "### Rules:\n",
    "- Replace each entity consistently throughout the text\n",
    "- Replace person names with different names keeping same word count\n",
    "- Replace company names with different company names\n",
    "- Replace city/location names with different cities\n",
    "- Replace driver license to another driver license number\n",
    "- replace Bitcoin key to another bitcoin key\n",
    "- replace SSN number to another SSN number\n",
    "- replace password to a different password\n",
    "- Keep email addresses in proper format without spaces\n",
    "- Keep alphanumeric keys in exact format length\n",
    "- Maintain original sentence or paragraph structure and format\n",
    "- return the replaced entities also in bracket\n",
    "- DO not rewrite sentences where is no replacement happened, return them as it is!\n",
    "- Do not add any explanations or comments\n",
    "\n",
    "### Response:\"\"\"}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Format prompt for vLLM\n",
    "        prompt = messages[0][\"content\"] + \"\\n\\n\" + messages[1][\"content\"]\n",
    "        \n",
    "        # Generate using vLLM\n",
    "        outputs = llm.generate([prompt], sampling_params)\n",
    "        replaced_text = outputs[0].outputs[0].text\n",
    "        \n",
    "        # Clean up the output\n",
    "        replaced_text = clean_output(replaced_text)\n",
    "        \n",
    "        # If model didn't make proper replacements, use fallback\n",
    "        if replaced_text == '' or replaced_text == text:\n",
    "            print('fallback!\\n')\n",
    "            return apply_fallback_replacements(text, pii_tracker)\n",
    "            \n",
    "        return replaced_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in replacement: {str(e)}\")\n",
    "        return apply_fallback_replacements(text, pii_tracker)\n",
    "\n",
    "def process_pii_pipeline(text):\n",
    "    \"\"\"\n",
    "    Process text with PII tracking and grouped entities\n",
    "    \"\"\"\n",
    "    # First use BERT to get masked text\n",
    "    masked_text, initial_mapping = step1_bert_ner(text)\n",
    "    \n",
    "    # Create bracketed version\n",
    "    bracketed_text = text\n",
    "    processed_spans = set()\n",
    "    \n",
    "    # Sort entities by length and create a list of tuples for easier processing\n",
    "    sorted_entities = []\n",
    "    for mask, info in initial_mapping.items():\n",
    "        original = info['original']\n",
    "        cleaned = re.sub(r'\\s*([\\'\"])\\s*', r'\\1', original)\n",
    "        sorted_entities.append({\n",
    "            'original': cleaned,\n",
    "            'type': info['type']\n",
    "        })\n",
    "    \n",
    "    # Sort by length (longest first) to handle overlapping entities properly\n",
    "    sorted_entities.sort(key=lambda x: -len(x['original']))\n",
    "    \n",
    "    # Create a list of all positions to replace\n",
    "    replacements = []\n",
    "    for entity in sorted_entities:\n",
    "        original = entity['original']\n",
    "        start = 0\n",
    "        text_len = len(bracketed_text)\n",
    "        \n",
    "        # Find all occurrences efficiently\n",
    "        while start < text_len:\n",
    "            pos = bracketed_text.find(original, start)\n",
    "            if pos == -1:\n",
    "                break\n",
    "                \n",
    "            end = pos + len(original)\n",
    "            # Check if this span overlaps with any processed spans\n",
    "            if not any(p[0] <= pos < p[1] or p[0] < end <= p[1] for p in processed_spans):\n",
    "                replacements.append((pos, end, original))\n",
    "                processed_spans.add((pos, end))\n",
    "            start = pos + 1\n",
    "    \n",
    "    # Sort replacements in reverse order to maintain correct positions\n",
    "    replacements.sort(key=lambda x: -x[0])\n",
    "    \n",
    "    # Apply all replacements at once\n",
    "    for start, end, original in replacements:\n",
    "        bracketed_text = (\n",
    "            bracketed_text[:start] + \n",
    "            f\"[{original}]\" + \n",
    "            bracketed_text[end:]\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        'original': text,\n",
    "        'bracketed': bracketed_text,\n",
    "        'masked': masked_text,\n",
    "        'mapping': initial_mapping\n",
    "    }\n",
    "def extract_email_parts(text):\n",
    "    \"\"\"\n",
    "    Extract the email content and category from the instruction-based format\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract the input section (email content)\n",
    "        input_match = re.search(r'### Input:\\n(.*?)(?=\\n### Response:)', text, re.DOTALL)\n",
    "        \n",
    "        email_content = input_match.group(1).strip() if input_match else \"\"\n",
    "        \n",
    "        # Extract the response section (category)\n",
    "        response_match = re.search(r'### Response:\\n?(.*?)(?:\\n|$)', text, re.DOTALL)\n",
    "        category = response_match.group(1).strip() if response_match else \"Uncategorized\"\n",
    "        \n",
    "        return email_content, category\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction error: {str(e)}\")\n",
    "        return text, \"Uncategorized\"\n",
    "\n",
    "def prepare_replacement_prompt(text: str) -> str:\n",
    "    \"\"\"Prepare prompt for entity replacement using the original template\"\"\"\n",
    "    system_msg = \"You are a specialized entity replacement system. Replace ALL marked entities in [brackets] with new consistent values.\"\n",
    "    user_msg = f\"\"\"Replace ALL entities marked with [brackets] in this text. Each entity MUST be replaced.\n",
    "\n",
    "### Input:\n",
    "{text}\n",
    "### Examples:\n",
    "Input: [John Smith] works at [Microsoft]. [John] enjoys [Seattle]. At [Microsoft], [Smith] is happy.\n",
    "Output: [Michael Brown] works at [TechCorp]. [Michael] enjoys [Boston]. At [TechCorp], [Brown] is happy.\n",
    "\n",
    "### Rules:\n",
    "- Replace each entity consistently throughout the text\n",
    "- Replace person names with different names keeping same word count\n",
    "- Replace company names with different company names\n",
    "- Replace city/location names with different cities\n",
    "- Replace driver license to another driver license number\n",
    "- replace Bitcoin key to another bitcoin key\n",
    "- replace SSN number to another SSN number\n",
    "- replace password to a different password\n",
    "- Keep email addresses in proper format without spaces\n",
    "- Keep alphanumeric keys in exact format length\n",
    "- Maintain original sentence or paragraph structure and format\n",
    "- Only return the replaced text, no explanations or lists\n",
    "- DO NOT include any \"Replacements:\" section\n",
    "- DO NOT add any additional comments or sections\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "    return f\"{system_msg}\\n\\n{user_msg}\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_prompts(prompts, output_dir):\n",
    "    \"\"\"Save prompts to a file\"\"\"\n",
    "    prompts_file = output_dir / \"prompts.jsonl\"\n",
    "    with jsonlines.open(prompts_file, mode='w') as writer:\n",
    "        for prompt in prompts:\n",
    "            writer.write({\"prompt\": prompt})\n",
    "    return prompts_file\n",
    "\n",
    "def load_processed_indices(output_dir):\n",
    "    \"\"\"Load indices of already processed prompts\"\"\"\n",
    "    final_output_file = output_dir / \"3_final_output.jsonl\"\n",
    "    processed_indices = set()\n",
    "    if final_output_file.exists():\n",
    "        with jsonlines.open(final_output_file) as reader:\n",
    "            for idx, _ in enumerate(reader):\n",
    "                processed_indices.add(idx)\n",
    "    return processed_indices\n",
    "\n",
    "def process_jsonl_with_intermediate_files(input_file, output_dir, batch_size=32, overwrite=True):\n",
    "    \"\"\"\n",
    "    Process JSONL file with intermediate files for each step\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Define intermediate file paths\n",
    "    original_file = output_dir / \"1_original.jsonl\"\n",
    "    bracketed_file = output_dir / \"2_bracketed.jsonl\"\n",
    "    final_output_file = output_dir / \"3_final_output.jsonl\"\n",
    "    prompts_file = output_dir / \"prompts.jsonl\"\n",
    "    \n",
    "    # Step 1 & 2: Extract and create bracketed versions if needed\n",
    "    if overwrite or not bracketed_file.exists():\n",
    "        print(\"Step 1 & 2: Processing original text and creating bracketed versions...\")\n",
    "        contents_and_categories = []\n",
    "        prompts = []\n",
    "        \n",
    "        with jsonlines.open(input_file) as reader, \\\n",
    "             jsonlines.open(original_file, mode='w') as original_writer, \\\n",
    "             jsonlines.open(bracketed_file, mode='w') as bracketed_writer:\n",
    "            \n",
    "            for item in tqdm(reader, desc=\"Processing entities\"):\n",
    "                text = item.get('text', '')\n",
    "                if text:\n",
    "                    content, category = extract_email_parts(text)\n",
    "                    if content:\n",
    "                        # Save original\n",
    "                        original_item = {\n",
    "                            'category': category,\n",
    "                            'text': content\n",
    "                        }\n",
    "                        original_writer.write(original_item)\n",
    "                        contents_and_categories.append(original_item)\n",
    "                        \n",
    "                        # Process and save bracketed version\n",
    "                        try:\n",
    "                            results = process_pii_pipeline(content)\n",
    "                            bracketed_item = {\n",
    "                                'category': category,\n",
    "                                'text': results['bracketed']\n",
    "                            }\n",
    "                            bracketed_writer.write(bracketed_item)\n",
    "                            \n",
    "                            # Prepare prompt\n",
    "                            prompt = prepare_replacement_prompt(results['bracketed'])\n",
    "                            prompts.append(prompt)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in bracketing: {str(e)}\")\n",
    "                            bracketed_writer.write(original_item)\n",
    "                            prompts.append(prepare_replacement_prompt(content))\n",
    "        \n",
    "        # Save prompts\n",
    "        save_prompts(prompts, output_dir)\n",
    "    \n",
    "    # Step 3: Generate final replacements using vLLM in batches\n",
    "    print(\"\\nStep 3: Generating final replacements...\")\n",
    "    \n",
    "    # Load prompts\n",
    "    all_prompts = []\n",
    "    with jsonlines.open(prompts_file) as reader:\n",
    "        for item in reader:\n",
    "            all_prompts.append(item['prompt'])\n",
    "    \n",
    "    # Get already processed indices\n",
    "    processed_indices = load_processed_indices(output_dir)\n",
    "    \n",
    "    # Process in batches\n",
    "    total_prompts = len(all_prompts)\n",
    "    for batch_start in range(0, total_prompts, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_prompts)\n",
    "        \n",
    "        # Skip if all items in this batch are already processed\n",
    "        if all(i in processed_indices for i in range(batch_start, batch_end)):\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing batch {batch_start//batch_size + 1}/{(total_prompts + batch_size - 1)//batch_size}\")\n",
    "        \n",
    "        # Get batch prompts\n",
    "        batch_prompts = all_prompts[batch_start:batch_end]\n",
    "        \n",
    "        try:\n",
    "            # Generate using vLLM\n",
    "            outputs = llm.generate(batch_prompts, sampling_params)\n",
    "            \n",
    "            # Save results\n",
    "            with jsonlines.open(final_output_file, mode='a') as writer:\n",
    "                for idx, output in enumerate(outputs):\n",
    "                    global_idx = batch_start + idx\n",
    "                    if global_idx not in processed_indices:\n",
    "                        try:\n",
    "                            replaced_text = output.outputs[0].text.strip()\n",
    "                            replaced_text = clean_output(replaced_text)\n",
    "                            \n",
    "                            # Load corresponding original item\n",
    "                            with jsonlines.open(bracketed_file) as reader:\n",
    "                                for i, item in enumerate(reader):\n",
    "                                    if i == global_idx:\n",
    "                                        output_item = {\n",
    "                                            'category': item['category'],\n",
    "                                            'text': replaced_text if replaced_text else item['text']\n",
    "                                        }\n",
    "                                        writer.write(output_item)\n",
    "                                        break\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in final output for index {global_idx}: {str(e)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_start//batch_size + 1}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nProcessing complete. Results saved in {output_dir}\")\n",
    "    print(f\"1. Original content: {original_file}\")\n",
    "    print(f\"2. Bracketed versions: {bracketed_file}\")\n",
    "    print(f\"3. Final output: {final_output_file}\")\n",
    "    print(f\"4. Prompts: {prompts_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_jsonl = \"data/costco_corporate_synthetic_emails.jsonl\"\n",
    "    #input_jsonl = 'test.jsonl'\n",
    "    output_jsonl = \"data\"\n",
    "    process_jsonl_with_intermediate_files(input_jsonl, output_jsonl,\n",
    "                                          batch_size=600,\n",
    "        overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
